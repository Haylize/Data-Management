# page_wordcloud.py
import re
import streamlit as st
import matplotlib.pyplot as plt
from wordcloud import WordCloud


DEFAULT_STOPWORDS_FR = {
    "le","la","les","un","une","des","de","du","au","aux","en","et","ou","oÃ¹","que","qui",
    "dans","pour","par","avec","sans","sur","ce","cet","cette","ces","se","ses","son","sa",
    "est","sont","Ã©tÃ©","Ãªtre","avoir","ont","a","avait","plus","moins","comme",
    "ne","pas","encore","trÃ¨s","ainsi","entre","depuis","vers",
    "l","d","s","qu","Ã ","afin","toute","tous","toutes","leur","leurs","dont","lorsque","car"
}


@st.cache_data(show_spinner=False)
def load_text(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def preprocess_text(article: str) -> str:
    # 1) minuscules
    text = article.lower()

    # 2) suppression ponctuation / caractÃ¨res spÃ©ciaux (garde accents + apostrophe)
    text = re.sub(r"[^a-zÃ Ã¢Ã§Ã©Ã¨ÃªÃ«Ã®Ã¯Ã´Ã»Ã¹Ã¼Ã¿Ã¦Å“\s']", " ", text)

    # 3) espaces multiples
    text = re.sub(r"\s+", " ", text).strip()
    return text


def tokenize_filter(text: str, stopwords: set[str], min_len: int = 3) -> list[str]:
    tokens = text.split()
    clean_tokens = [w for w in tokens if (w not in stopwords and len(w) >= min_len)]
    return clean_tokens


@st.cache_data(show_spinner=False)
def build_wordcloud(
    text_for_wc: str,
    width: int,
    height: int,
    background: str,
    max_words: int,
    collocations: bool,
) -> WordCloud:
    wc = WordCloud(
        width=width,
        height=height,
        background_color=background,
        max_words=max_words,
        collocations=collocations,  # Ã©vite des bigrammes parfois bizarres
    ).generate(text_for_wc)
    return wc


def wordcloud_page() -> None:
    st.header("â˜ï¸ WordCloud â€” Article OMS")

    # chemin
    TEXT_PATH = "data/article_oms.txt"

    try:
        article = load_text(TEXT_PATH)
    except FileNotFoundError:
        st.error(f"Fichier introuvable : {TEXT_PATH}")
        st.stop()

    with st.expander("ðŸ”Ž AperÃ§u de lâ€™article", expanded=False):
        st.write(article[:1200] + ("..." if len(article) > 1200 else ""))

    # --- PrÃ©traitement
    cleaned = preprocess_text(article)

    # --- ParamÃ¨tres UI
    st.sidebar.subheader("âš™ï¸ WordCloud")
    min_len = st.sidebar.slider("Longueur minimale des mots", 2, 8, 3)
    max_words = st.sidebar.slider("Nombre max de mots", 50, 400, 200, step=25)
    background = st.sidebar.selectbox("Fond", ["white", "black"])
    collocations = st.sidebar.checkbox("Autoriser collocations (bigrams)", value=False)

    extra_stopwords = st.sidebar.text_area(
        "Stopwords supplÃ©mentaires (sÃ©parÃ©s par virgules)",
        value="",
        placeholder="ex: santÃ©, mondiale, rapport",
    )

    stopwords = set(DEFAULT_STOPWORDS_FR)
    if extra_stopwords.strip():
        stopwords |= {w.strip().lower() for w in extra_stopwords.split(",") if w.strip()}

    clean_tokens = tokenize_filter(cleaned, stopwords=stopwords, min_len=min_len)

    col1, col2 = st.columns(2)
    col1.metric("Tokens bruts", len(cleaned.split()))
    col2.metric("Tokens nettoyÃ©s", len(clean_tokens))

    if not clean_tokens:
        st.warning("AprÃ¨s filtrage, il ne reste aucun mot. Diminue les stopwords ou la longueur minimale.")
        st.stop()

    text_for_wc = " ".join(clean_tokens)

    # --- GÃ©nÃ©ration + affichage
    wc = build_wordcloud(
        text_for_wc=text_for_wc,
        width=1000,
        height=500,
        background=background,
        max_words=max_words,
        collocations=collocations,
    )

    fig, ax = plt.subplots(figsize=(12, 6))
    ax.imshow(wc, interpolation="bilinear")
    ax.axis("off")
    st.pyplot(fig, use_container_width=True)

    st.subheader("ðŸ§  Analyse du nuage de mots")

    st.markdown(
        """
    Le nuage de mots met en Ã©vidence les termes les plus frÃ©quents de l'article de l'OMS. 
    On observe notamment la prÃ©sence de mots liÃ©s Ã  la santÃ© mondiale et aux indicateurs sanitaires 
    (par exemple : sante, *progres, deces, sanitaire, oms, etc.).
    Cela confirme que l'article traite principalement du ralentissement des progrÃ¨s en matiÃ¨re de santÃ© au niveau mondial et des inÃ©galitÃ©s persistantes entre les rÃ©gions. 
    Cette analyse textuelle complÃ¨te donc l'exploration de notre jeu de donnÃ©es de statistiques de santÃ© globale.

    Cette analyse textuelle complÃ¨te les graphiques et les statistiques de lâ€™application en
    apportant une lecture plus qualitative du contenu du rapport, et permet de mieux
    comprendre le contexte global des donnÃ©es analysÃ©es.
    """
    )


    # --- Download
    png_bytes = wc.to_image()
    # to_image() renvoie une PIL.Image -> on convertit en bytes
    import io
    buf = io.BytesIO()
    png_bytes.save(buf, format="PNG")
    st.download_button(
        "ðŸ“¥ TÃ©lÃ©charger le WordCloud (PNG)",
        data=buf.getvalue(),
        file_name="wordcloud_oms.png",
        mime="image/png",
    )
